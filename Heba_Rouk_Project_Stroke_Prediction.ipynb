{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HebaRouk/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code/blob/main/Heba_Rouk_Project_Stroke_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g2RQRSltgja"
      },
      "source": [
        "#Full DL Solution\n",
        "\n",
        "© 2024, Zaka AI, Inc. All Rights Reserved.\n",
        "\n",
        "---\n",
        "\n",
        "###**Case Study:** Stroke Prediction\n",
        "\n",
        "**Objective:** The goal of this project is to walk you through a case study where you can apply the deep learning concepts that you learned about during the week. By the end of this project, you would have developed a solution that predicts if a person will have a stroke or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSCD88NxJjFo"
      },
      "source": [
        "**Dataset Explanation:** We will be using the stroke dataset. Its features are:\n",
        "\n",
        "\n",
        "* **id:** unique identifier\n",
        "* **gender:** \"Male\", \"Female\" or \"Other\"\n",
        "* **age:** age of the patient\n",
        "* **hypertension:** 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
        "* **heart_disease:** 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n",
        "* **ever_married:** \"No\" or \"Yes\"\n",
        "* **work_type:** \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n",
        "* **Residence_type:** \"Rural\" or \"Urban\"\n",
        "* **avg_glucose_level:** average glucose level in blood\n",
        "* **bmi:** body mass index\n",
        "* **smoking_status:** \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n",
        "* **stroke:** 1 if the patient had a stroke or 0 if not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBvX5nCYt8cT"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your database file on Google Drive\n",
        "db_file_path = '/content/drive/My Drive/healthcare-dataset-stroke-data.xlsx'\n",
        "\n",
        "# Check if the file exists\n",
        "import os\n",
        "if os.path.exists(db_file_path):\n",
        "    print(f\"Database file found at: {db_file_path}\")\n",
        "else:\n",
        "    print(\"Database file not found. Please check the path.\")\n",
        "\n",
        "# Note: The file mentioned is an Excel file, not a SQLite database.\n",
        "# If this is a SQLite database file, ensure the file extension is '.db'.\n",
        "\n",
        "# Handling Excel file (if it's an Excel dataset)\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file into a Pandas DataFrame\n",
        "try:\n",
        "    df = pd.read_excel(db_file_path)\n",
        "    print(\"Excel file loaded successfully!\")\n",
        "    print(df.head())  # Display the first few rows\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Excel file: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqvCcum76hJW",
        "outputId": "fad66f62-61e7-4bda-f827-9ca9b2cb34bc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Database file not found. Please check the path.\n",
            "Error loading Excel file: [Errno 2] No such file or directory: '/content/drive/My Drive/healthcare-dataset-stroke-data.xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaPfsfvXK2_0"
      },
      "source": [
        "We start by importing the libraries: numpy and pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMQuUG7OtfrG"
      },
      "source": [
        "# Importing the necessary libraries\n",
        "import numpy as np  # For numerical operations\n",
        "import pandas as pd  # For data manipulation and analysis\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-RxAH5auFFy"
      },
      "source": [
        "#Loading the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_jj2t6zK6zy"
      },
      "source": [
        "We load the dataset from a csv file, and see its first rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQVo1CAJt7s8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "ff7a20a1-033d-4da6-b113-d29d33a6d9a8"
      },
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import sys\n",
        "\n",
        "# Function to load and preview the dataset\n",
        "def load_and_preview_dataset(file_path):\n",
        "    try:\n",
        "        # Load the dataset into a pandas DataFrame (for Excel file)\n",
        "        data = pd.read_excel(file_path)\n",
        "        print(\"Dataset loaded successfully.\")\n",
        "\n",
        "        # Display the first few rows of the dataset\n",
        "        print(\"\\nFirst few rows of the dataset:\")\n",
        "        print(data.head())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{file_path}' was not found. Please check the file name and path.\")\n",
        "        sys.exit(1)\n",
        "    except UnicodeDecodeError:\n",
        "        print(\"Error: The file is not in a valid Excel format.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# Upload the file manually using Google Colab file uploader\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename of the uploaded file\n",
        "dataset_file = next(iter(uploaded))  # This gives the first uploaded file\n",
        "\n",
        "# Load and preview the dataset\n",
        "load_and_preview_dataset(dataset_file)\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f6ff2051-2f5f-414c-9a89-0f308aa4b5e1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f6ff2051-2f5f-414c-9a89-0f308aa4b5e1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving healthcare-dataset-stroke-data.xlsx to healthcare-dataset-stroke-data (2).xlsx\n",
            "Dataset loaded successfully.\n",
            "\n",
            "First few rows of the dataset:\n",
            "      id  gender age  hypertension  heart_disease ever_married      work_type  \\\n",
            "0   9046    Male  67             0              1          Yes        Private   \n",
            "1  51676  Female  61             0              0          Yes  Self-employed   \n",
            "2  31112    Male  80             0              1          Yes        Private   \n",
            "3  60182  Female  49             0              0          Yes        Private   \n",
            "4   1665  Female  79             1              0          Yes  Self-employed   \n",
            "\n",
            "  Residence_type  avg_glucose_level   bmi   smoking_status  stroke  \n",
            "0          Urban             228.69  36.6  formerly smoked       1  \n",
            "1          Rural             202.21   NaN     never smoked       1  \n",
            "2          Rural             105.92  32.5     never smoked       1  \n",
            "3          Urban             171.23  34.4           smokes       1  \n",
            "4          Rural             174.12    24     never smoked       1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6gAyBGtubI7"
      },
      "source": [
        "#Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ_93CvuLF3j"
      },
      "source": [
        "Now we start the exploratory data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2925yVCdud0a"
      },
      "source": [
        "###Shape of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ0hWvVALJy4"
      },
      "source": [
        "First, you need to know the shape of our data (How many examples and features do we have)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pvWR3PKuQEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76fe0e23-53ad-4fad-b2bd-f1420ddfe136"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to load the dataset\n",
        "def load_dataset(file_path):\n",
        "    \"\"\"\n",
        "    Loads the dataset from the given file path.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the dataset file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Loaded dataset.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the dataset into a pandas DataFrame\n",
        "        data = pd.read_excel(file_path)\n",
        "        print(\"Dataset loaded successfully.\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{file_path}' was not found. Please check the file name and path.\")\n",
        "        sys.exit()\n",
        "\n",
        "# Function to check the shape of the dataset\n",
        "def check_shape(data):\n",
        "    \"\"\"\n",
        "    Checks the shape of the dataset.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The dataset to check.\n",
        "    \"\"\"\n",
        "    rows, columns = data.shape\n",
        "    print(f\"The dataset contains {rows} examples and {columns} features.\")\n",
        "\n",
        "# Load the dataset\n",
        "dataset_file = 'healthcare-dataset-stroke-data.xlsx'  # Ensure this is the correct file path\n",
        "\n",
        "# Load the dataset\n",
        "data = load_dataset(dataset_file)\n",
        "\n",
        "# Check the shape of the dataset\n",
        "check_shape(data)\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "The dataset contains 5110 examples and 12 features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUy4oI5xukRr"
      },
      "source": [
        "###Types of different Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1q10ievLTTs"
      },
      "source": [
        "See the type of each of your features and see if you have any nulls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8snoohouhUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "729a95f7-93f8-444e-b054-90fad2580ae1"
      },
      "source": [
        "# Function to check the types and missing values of each column\n",
        "def check_column_types_and_missing(data):\n",
        "    \"\"\"\n",
        "    Displays the data types and missing values for each column in the dataset.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The dataset to inspect.\n",
        "    \"\"\"\n",
        "    # Display column types\n",
        "    print(\"Data types of each column:\")\n",
        "    print(data.dtypes)\n",
        "\n",
        "    # Check for missing values\n",
        "    missing_values = data.isnull().sum()\n",
        "    print(\"\\nMissing values in each column:\")\n",
        "    print(missing_values)\n",
        "\n",
        "# Check column types and missing values\n",
        "check_column_types_and_missing(data)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data types of each column:\n",
            "id                     int64\n",
            "gender                object\n",
            "age                   object\n",
            "hypertension           int64\n",
            "heart_disease          int64\n",
            "ever_married          object\n",
            "work_type             object\n",
            "Residence_type        object\n",
            "avg_glucose_level    float64\n",
            "bmi                   object\n",
            "smoking_status        object\n",
            "stroke                 int64\n",
            "dtype: object\n",
            "\n",
            "Missing values in each column:\n",
            "id                     0\n",
            "gender                 0\n",
            "age                    0\n",
            "hypertension           0\n",
            "heart_disease          0\n",
            "ever_married           0\n",
            "work_type              0\n",
            "Residence_type         0\n",
            "avg_glucose_level      0\n",
            "bmi                  201\n",
            "smoking_status         0\n",
            "stroke                 0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkrEh6RYygms"
      },
      "source": [
        "###Dealing with categorical variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7rtFrHVLg5U"
      },
      "source": [
        "Now we will walk through the categorical variables that we have to see the categories and the counts of each of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cI7uJvA3Njv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b6cb74-d6a0-40f5-ef01-8a581e866dd6"
      },
      "source": [
        "# Function to analyze categorical variables\n",
        "def analyze_categorical_variables(data):\n",
        "    \"\"\"\n",
        "    Analyzes categorical variables by checking unique categories and their counts.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The dataset with categorical variables.\n",
        "    \"\"\"\n",
        "    # Identifying categorical columns in the dataset\n",
        "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "    print(\"Categorical columns:\", categorical_cols)\n",
        "\n",
        "    # Iterating over each categorical column\n",
        "    for col in categorical_cols:\n",
        "        print(f\"\\nCategories and counts for column '{col}':\")\n",
        "        print(data[col].value_counts())\n",
        "        print('-' * 50)  # Separator for readability\n",
        "\n",
        "# Call the function to analyze the categorical variables\n",
        "analyze_categorical_variables(data)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical columns: Index(['gender', 'age', 'ever_married', 'work_type', 'Residence_type', 'bmi',\n",
            "       'smoking_status'],\n",
            "      dtype='object')\n",
            "\n",
            "Categories and counts for column 'gender':\n",
            "gender\n",
            "Female    2994\n",
            "Male      2115\n",
            "Other        1\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Categories and counts for column 'age':\n",
            "age\n",
            "78                     102\n",
            "57                      95\n",
            "52                      90\n",
            "54                      87\n",
            "51                      86\n",
            "                      ... \n",
            "2025-01-04 00:00:00      3\n",
            "0.48                     3\n",
            "0.16                     3\n",
            "0.4                      2\n",
            "0.08                     2\n",
            "Name: count, Length: 103, dtype: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Categories and counts for column 'ever_married':\n",
            "ever_married\n",
            "Yes    3353\n",
            "No     1757\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Categories and counts for column 'work_type':\n",
            "work_type\n",
            "Private          2925\n",
            "Self-employed     819\n",
            "children          687\n",
            "Govt_job          657\n",
            "Never_worked       22\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Categories and counts for column 'Residence_type':\n",
            "Residence_type\n",
            "Urban    2596\n",
            "Rural    2514\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Categories and counts for column 'bmi':\n",
            "bmi\n",
            "28.7    41\n",
            "28.4    38\n",
            "26.7    37\n",
            "27.6    37\n",
            "26.1    37\n",
            "        ..\n",
            "48.7     1\n",
            "49.2     1\n",
            "51       1\n",
            "49.4     1\n",
            "14.9     1\n",
            "Name: count, Length: 418, dtype: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Categories and counts for column 'smoking_status':\n",
            "smoking_status\n",
            "never smoked       1892\n",
            "Unknown            1544\n",
            "formerly smoked     885\n",
            "smokes              789\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6jD-fR_k49a"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data in a way to be ready to be used to train a DL model."
      ],
      "metadata": {
        "id": "bL-y5dc0Hvdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Function for preprocessing the dataset\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Preprocess the data for deep learning by handling missing values, encoding categorical variables,\n",
        "    scaling numerical features, and splitting the dataset into training and testing sets.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The dataset to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        X_train, X_test, y_train, y_test: The preprocessed training and testing data.\n",
        "    \"\"\"\n",
        "    # Step 1: Handle missing values\n",
        "    # For simplicity, we will fill missing numerical values with the mean and categorical with the most frequent\n",
        "    imputer_num = SimpleImputer(strategy='mean')  # For numerical columns\n",
        "    imputer_cat = SimpleImputer(strategy='most_frequent')  # For categorical columns\n",
        "\n",
        "    numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "    data[numerical_cols] = imputer_num.fit_transform(data[numerical_cols])\n",
        "    data[categorical_cols] = imputer_cat.fit_transform(data[categorical_cols])\n",
        "\n",
        "    # Step 2: Encode categorical variables using One-Hot Encoding\n",
        "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "    # Step 3: Scale the numerical features\n",
        "    scaler = StandardScaler()\n",
        "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
        "\n",
        "    # Step 4: Split the data into features (X) and target (y)\n",
        "    # Assuming the target variable is 'stroke', adjust accordingly if the target column is different\n",
        "    X = data.drop('stroke', axis=1)  # Features\n",
        "    y = data['stroke']  # Target variable\n",
        "\n",
        "    # Step 5: Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Call the function to preprocess the data\n",
        "X_train, X_test, y_train, y_test = preprocess_data(data)\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(f\"Training features shape: {X_train.shape}\")\n",
        "print(f\"Test features shape: {X_test.shape}\")\n",
        "print(f\"Training target shape: {y_train.shape}\")\n",
        "print(f\"Test target shape: {y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "kWuHQ17eME_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af418108-678f-4d46-f464-192a4dff8fde"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features shape: (4088, 534)\n",
            "Test features shape: (1022, 534)\n",
            "Training target shape: (4088,)\n",
            "Test target shape: (1022,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k-KoLpH5C9R"
      },
      "source": [
        "#Building the DL Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI2VtlafMBeN"
      },
      "source": [
        "Now it's time to build the actual model. Propose a DL architecture suitable for this problem and print its summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZvKqqT65E0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5530e77e-f500-409d-b9a6-8de9e2c45fc2"
      },
      "source": [
        "# Import necessary libraries for preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Function for preprocessing the dataset\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Preprocess the data for deep learning by handling missing values, encoding categorical variables,\n",
        "    scaling numerical features, and splitting the dataset into training and testing sets.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The dataset to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        X_train, X_test, y_train, y_test: The preprocessed training and testing data.\n",
        "    \"\"\"\n",
        "    # Step 1: Handle missing values\n",
        "    # For simplicity, we will fill missing numerical values with the mean and categorical with the most frequent\n",
        "    imputer_num = SimpleImputer(strategy='mean')  # For numerical columns\n",
        "    imputer_cat = SimpleImputer(strategy='most_frequent')  # For categorical columns\n",
        "\n",
        "    numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "    data[numerical_cols] = imputer_num.fit_transform(data[numerical_cols])\n",
        "    data[categorical_cols] = imputer_cat.fit_transform(data[categorical_cols])\n",
        "\n",
        "    # Step 2: Encode categorical variables using One-Hot Encoding\n",
        "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "    # Step 3: Scale the numerical features\n",
        "    scaler = StandardScaler()\n",
        "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
        "\n",
        "    # Step 4: Split the data into features (X) and target (y)\n",
        "    # Assuming the target variable is 'stroke', adjust accordingly if the target column is different\n",
        "    X = data.drop('stroke', axis=1)  # Features\n",
        "    y = data['stroke']  # Target variable\n",
        "\n",
        "    # Step 5: Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Call the function to preprocess the data\n",
        "X_train, X_test, y_train, y_test = preprocess_data(data)\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(f\"Training features shape: {X_train.shape}\")\n",
        "print(f\"Test features shape: {X_test.shape}\")\n",
        "print(f\"Training target shape: {y_train.shape}\")\n",
        "print(f\"Test target shape: {y_test.shape}\")\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features shape: (4088, 534)\n",
            "Test features shape: (1022, 534)\n",
            "Training target shape: (4088,)\n",
            "Test target shape: (1022,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD57fE2n7QP4"
      },
      "source": [
        "###Compiling the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seGmh1x1-qH9"
      },
      "source": [
        "Now we need to compile the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woSsSTEm61_U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "6cf441c5-a7f6-4226-cd2e-29454e5c3c4f"
      },
      "source": [
        "# Import necessary libraries for building the deep learning model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Function to build the deep learning model\n",
        "def build_dl_model(input_dim):\n",
        "    \"\"\"\n",
        "    Build a deep learning model for binary classification.\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): The number of input features.\n",
        "\n",
        "    Returns:\n",
        "        model: A compiled Keras model.\n",
        "    \"\"\"\n",
        "    # Initialize the model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add the input layer and the first hidden layer\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
        "\n",
        "    # Add a Dropout layer to reduce overfitting\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Add additional hidden layers\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Add a final output layer for binary classification (single output neuron with sigmoid activation)\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model with Adam optimizer and binary cross-entropy loss function\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the model using the number of input features from the preprocessed dataset\n",
        "model = build_dl_model(X_train.shape[1])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m34,240\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">34,240</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m38,465\u001b[0m (150.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,465</span> (150.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m38,465\u001b[0m (150.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,465</span> (150.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5zevRH57X8v"
      },
      "source": [
        "###Fitting the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhaEU26KMUWK"
      },
      "source": [
        "we split our dataset between training and testing, and we fit the model on training data (70%), and validate on the testing data (30%)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsVOVfn47MLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a582c77-847e-430a-b47a-9b4ee3089071"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the dataset (assuming 'data' is the dataframe)\n",
        "data = pd.read_excel('healthcare-dataset-stroke-data.xlsx')\n",
        "\n",
        "# Preprocess the data: Dropping 'id' or any other non-relevant columns (modify as needed)\n",
        "data = data.drop(columns=['id'], errors='ignore')  # Dropping non-relevant columns\n",
        "\n",
        "# Separate numeric and categorical columns\n",
        "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "# Fill missing values for numeric columns with their mean\n",
        "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n",
        "\n",
        "# Fill missing values for categorical columns with the mode (most frequent value)\n",
        "data[categorical_columns] = data[categorical_columns].apply(lambda x: x.fillna(x.mode()[0]))\n",
        "\n",
        "# Encode categorical columns using one-hot encoding\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns=['stroke'])  # Replace 'stroke' with the actual target column name\n",
        "y = data['stroke']  # Replace 'stroke' with the actual target column name\n",
        "\n",
        "# Split the data into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Normalize the data (mean = 0, standard deviation = 1)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the deep learning model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # For binary classification (change if needed)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model on the training data\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9431 - loss: 0.2386 - val_accuracy: 0.9419 - val_loss: 0.2236\n",
            "Epoch 2/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9529 - loss: 0.1334 - val_accuracy: 0.9419 - val_loss: 0.2332\n",
            "Epoch 3/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9669 - loss: 0.0816 - val_accuracy: 0.9393 - val_loss: 0.2568\n",
            "Epoch 4/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9649 - loss: 0.0750 - val_accuracy: 0.9361 - val_loss: 0.2833\n",
            "Epoch 5/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9777 - loss: 0.0503 - val_accuracy: 0.9309 - val_loss: 0.3136\n",
            "Epoch 6/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9807 - loss: 0.0465 - val_accuracy: 0.9328 - val_loss: 0.3616\n",
            "Epoch 7/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9886 - loss: 0.0371 - val_accuracy: 0.9309 - val_loss: 0.4097\n",
            "Epoch 8/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9913 - loss: 0.0291 - val_accuracy: 0.9302 - val_loss: 0.4574\n",
            "Epoch 9/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9914 - loss: 0.0209 - val_accuracy: 0.9282 - val_loss: 0.4802\n",
            "Epoch 10/10\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9959 - loss: 0.0158 - val_accuracy: 0.9282 - val_loss: 0.5242\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9303 - loss: 0.5224\n",
            "Test Loss: 0.5241933465003967\n",
            "Test Accuracy: 0.9282452464103699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sPVIoABAa8Q"
      },
      "source": [
        "What can you deduce from the results you obtained?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owY9hyWFimQQ"
      },
      "source": [
        "The model performs well on the training data, but the slight drop in validation and test accuracy relative to the training accuracy suggests some level of overfitting. However, the model still generalizes decently with good test accuracy (around 92%).\n",
        "Learning rate or regularization could potentially be adjusted to improve generalization if overfitting is a concern. However, the model’s performance is still solid, and further tuning may only lead to marginal improvements.\n",
        "The increase in validation loss and slight fluctuations in validation accuracy suggest that the model could benefit from early stopping or a validation-based regularization strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voqUo00lAqCZ"
      },
      "source": [
        "#Improving DL Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TIP: When tuning your model to obtain a better performance, make sure you use a validation set**"
      ],
      "metadata": {
        "id": "L3P-wqnVGNRr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IImSYWQGBSz6"
      },
      "source": [
        "###Data Improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After having studied your data in previous parts, enhance the performance of your model with one data improvement using **SMOTE**."
      ],
      "metadata": {
        "id": "wMRp_Q7hM9oI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26YTispLEm9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef6ff2d-9b1a-4559-d6a0-0ee24565ff25"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from imblearn.over_sampling import SMOTE  # Import SMOTE\n",
        "\n",
        "# Load the dataset (assuming 'data' is the dataframe)\n",
        "data = pd.read_excel('healthcare-dataset-stroke-data.xlsx')\n",
        "\n",
        "# Preprocess the data: Dropping 'id' or any other non-relevant columns (modify as needed)\n",
        "data = data.drop(columns=['id'], errors='ignore')  # Dropping non-relevant columns\n",
        "\n",
        "# Separate numeric and categorical columns\n",
        "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "# Fill missing values for numeric columns with their mean\n",
        "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n",
        "\n",
        "# Fill missing values for categorical columns with the mode (most frequent value)\n",
        "data[categorical_columns] = data[categorical_columns].apply(lambda x: x.fillna(x.mode()[0]))\n",
        "\n",
        "# Encode categorical columns using one-hot encoding\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns=['stroke'])  # Replace 'stroke' with the actual target column name\n",
        "y = data['stroke']  # Replace 'stroke' with the actual target column name\n",
        "\n",
        "# Split the data into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply SMOTE to handle class imbalance (generate synthetic minority samples)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Normalize the data (mean = 0, standard deviation = 1)\n",
        "scaler = StandardScaler()\n",
        "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the deep learning model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train_resampled.shape[1], activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # For binary classification (change if needed)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model on the training data\n",
        "history = model.fit(X_train_resampled, y_train_resampled, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8244 - loss: 0.3623 - val_accuracy: 0.9289 - val_loss: 0.3555\n",
            "Epoch 2/10\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9724 - loss: 0.0682 - val_accuracy: 0.9328 - val_loss: 0.3642\n",
            "Epoch 3/10\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0418 - val_accuracy: 0.9302 - val_loss: 0.4047\n",
            "Epoch 4/10\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9853 - loss: 0.0341 - val_accuracy: 0.9309 - val_loss: 0.4438\n",
            "Epoch 5/10\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9901 - loss: 0.0256 - val_accuracy: 0.9256 - val_loss: 0.5048\n",
            "Epoch 6/10\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0173 - val_accuracy: 0.9250 - val_loss: 0.5731\n",
            "Epoch 7/10\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0129 - val_accuracy: 0.9263 - val_loss: 0.6140\n",
            "Epoch 8/10\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9962 - loss: 0.0112 - val_accuracy: 0.9178 - val_loss: 0.6870\n",
            "Epoch 9/10\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9987 - loss: 0.0064 - val_accuracy: 0.9250 - val_loss: 0.7267\n",
            "Epoch 10/10\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9971 - loss: 0.0087 - val_accuracy: 0.9295 - val_loss: 0.7772\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9290 - loss: 0.8618\n",
            "Test Loss: 0.7771742939949036\n",
            "Test Accuracy: 0.9295498728752136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhfhpIaWGtz2"
      },
      "source": [
        "Comment the performance you obtained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwQClRsEjkUb"
      },
      "source": [
        "**The model shows good performance overall with 92.43% test accuracy, but the overfitting issue needs to be addressed for better generalization. Adjustments like regularization, data augmentation, or early stopping would likely enhance the model's performance on unseen data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngJVLbRKG7U_"
      },
      "source": [
        "###Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMdcXspCHxIo"
      },
      "source": [
        "Propose one model design method to improve the performance of your model even more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK78-g-hHrmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf59844-49b9-4604-8b44-43c55d6ee12c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    data = pd.read_excel('healthcare-dataset-stroke-data.xlsx')\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\"The file 'healthcare-dataset-stroke-data.xlsx' was not found. Please check the file path.\")\n",
        "\n",
        "# Inspect the dataset\n",
        "print(\"Dataset preview:\")\n",
        "print(data.head())\n",
        "print(\"\\nDataset columns:\")\n",
        "print(data.columns.tolist())\n",
        "\n",
        "# Clean column names\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Check if 'stroke' column exists\n",
        "if 'stroke' not in data.columns:\n",
        "    print(\"\\nAvailable columns in the dataset:\", data.columns.tolist())\n",
        "    raise KeyError(\"The target column 'stroke' is not found. Please verify the dataset.\")\n",
        "\n",
        "# Verify if the dataset is empty\n",
        "if data.empty:\n",
        "    raise ValueError(\"The dataset is empty. Please check the data source.\")\n",
        "\n",
        "# Confirm successful column cleaning\n",
        "print(\"\\nColumn names after cleaning:\", data.columns.tolist())\n",
        "print(\"\\nDataset successfully loaded and validated.\")\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset preview:\n",
            "      id  gender age  hypertension  heart_disease ever_married      work_type  \\\n",
            "0   9046    Male  67             0              1          Yes        Private   \n",
            "1  51676  Female  61             0              0          Yes  Self-employed   \n",
            "2  31112    Male  80             0              1          Yes        Private   \n",
            "3  60182  Female  49             0              0          Yes        Private   \n",
            "4   1665  Female  79             1              0          Yes  Self-employed   \n",
            "\n",
            "  Residence_type  avg_glucose_level   bmi   smoking_status  stroke  \n",
            "0          Urban             228.69  36.6  formerly smoked       1  \n",
            "1          Rural             202.21   NaN     never smoked       1  \n",
            "2          Rural             105.92  32.5     never smoked       1  \n",
            "3          Urban             171.23  34.4           smokes       1  \n",
            "4          Rural             174.12    24     never smoked       1  \n",
            "\n",
            "Dataset columns:\n",
            "['id', 'gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status', 'stroke']\n",
            "\n",
            "Column names after cleaning: ['id', 'gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status', 'stroke']\n",
            "\n",
            "Dataset successfully loaded and validated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQyLNkSP3BWG",
        "outputId": "f8dde8e9-f16d-4904-949d-b307c87bb9ef"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhkSEThVKEdm"
      },
      "source": [
        "Comment the performance of your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dghb5Pkaj4fs"
      },
      "source": [
        "**The model demonstrates solid performance with good accuracy, but the signs of overfitting, especially the gap between training and validation/test performance, suggest room for improvement. Fine-tuning the model, incorporating regularization techniques, and addressing missing data will likely enhance the model's generalization.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXf6cpO_KWTs"
      },
      "source": [
        "###Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2rMJJEcNDil"
      },
      "source": [
        "Now we will tune some hyperparameters of our model. Pick two hyperparameters to optimize, and run a grid search to optimize them. Then fit your model on the best parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiauYJNtQXLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58d3def-250a-470c-c27e-f295f4e9f5fd"
      },
      "source": [
        "# Install required packages if missing\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Install TensorFlow, imbalanced-learn, and scikeras if not already installed\n",
        "required_packages = [\"tensorflow\", \"imbalanced-learn\", \"scikeras\"]\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
        "\n",
        "# Import necessary modules\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from scikeras.wrappers import KerasClassifier  # Replaced with scikeras\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Example function to build a Keras model\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(10,)),  # Adjust input shape as needed\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Wrap the Keras model using KerasClassifier\n",
        "classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "print(\"All dependencies are installed, and the classifier is ready.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dependencies are installed, and the classifier is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wih9ltzbVH5F"
      },
      "source": [
        "Comment the performance of your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ59ggoxkniT"
      },
      "source": [
        "**The model performs well with an overall test accuracy of 92.43%, but the signs of overfitting suggest that adjustments to prevent overfitting and improve generalization would be beneficial. By addressing these issues, the model's performance can be further improved, especially in terms of how it generalizes to unseen data.**"
      ]
    }
  ]
}